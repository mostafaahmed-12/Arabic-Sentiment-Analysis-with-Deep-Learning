# -*- coding: utf-8 -*-
"""NeuralNetworkFinall.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16fXbSqI589yYmFSUSR6p7S1qheQVcIq3
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install NLTK

# %pip install Arabic-Stopwords

# %pip install clean-text

# %pip install demoji

from nltk import word_tokenize
from nltk import sent_tokenize
#import arabicstopwords.arabicstopwords as stp
from nltk.corpus import stopwords
#from cleantext import clean
from nltk.stem import ISRIStemmer
import pandas as pd
import re
import string
import sys
#import demoji
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
import pandas as pd
import re
#import emoji
from nltk.corpus import stopwords
import numpy as np
import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout
from keras.callbacks import EarlyStopping
import numpy as np
from keras.callbacks import LearningRateScheduler
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping

nltk.download('punkt')
nltk.download('stopwords')

excel_data = pd.read_excel('train.xlsx')
train_data = pd.DataFrame(excel_data,columns=['review_description', 'rating'])

ax = train_data['rating'].value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Class Distribution")

train_data.review_description.duplicated().sum()

train_data.review_description.isnull().sum()

train_data.drop(train_data[train_data.review_description.duplicated() == True].index, axis = 0, inplace = True)

train_data.review_description.duplicated().sum()

train_data.review_description=train_data.review_description.astype(str)
train_data.review_description=train_data.review_description.apply(lambda x:re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~"""), ' ', x))
train_data.review_description=train_data.review_description.apply(lambda x:x.replace('Ø›',"", ))

train_data.head(5)

emojis = {
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "ğŸ˜‚":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ’”":"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†",
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "â¤":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø¨",
    "ğŸ˜­":"ÙŠØ¨ÙƒÙŠ",
    "ğŸ˜¢":"Ø­Ø²Ù†",
    "ğŸ˜”":"Ø­Ø²Ù†",
    "â™¥":"Ø­Ø¨",
    "ğŸ’œ":"Ø­Ø¨",
    "ğŸ˜…":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ™":"Ø­Ø²ÙŠÙ†",
    "ğŸ’•":"Ø­Ø¨",
    "ğŸ’™":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜Š":"Ø³Ø¹Ø§Ø¯Ø©",
    "ğŸ‘":"ÙŠØµÙÙ‚",
    "ğŸ‘Œ":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜´":"ÙŠÙ†Ø§Ù…",
    "ğŸ˜€":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜Œ":"Ø­Ø²ÙŠÙ†",
    "ğŸŒ¹":"ÙˆØ±Ø¯Ø©",
    "ğŸ™ˆ":"Ø­Ø¨",
    "ğŸ˜„":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜":"Ù…Ø­Ø§ÙŠØ¯",
    "âœŒ":"Ù…Ù†ØªØµØ±",
    "âœ¨":"Ù†Ø¬Ù…Ù‡",
    "ğŸ¤”":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ˜’":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ™„":"Ù…Ù„Ù„",
    "ğŸ˜•":"Ø¹ØµØ¨ÙŠØ©",
    "ğŸ˜ƒ":"ÙŠØ¶Ø­Ùƒ",
    "ğŸŒ¸":"ÙˆØ±Ø¯Ø©",
    "ğŸ˜“":"Ø­Ø²Ù†",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ’—":"Ø­Ø¨",
    "ğŸ˜‘":"Ù…Ù†Ø²Ø¹Ø¬",
    "ğŸ’­":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"Ø«Ù‚Ø©",
    "ğŸ’›":"Ø­Ø¨",
    "ğŸ˜©":"Ø­Ø²ÙŠÙ†",
    "ğŸ’ª":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ‘":"Ù…ÙˆØ§ÙÙ‚",
    "ğŸ™ğŸ»":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ˜³":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ‘ğŸ¼":"ØªØµÙÙŠÙ‚",
    "ğŸ¶":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸŒš":"ØµÙ…Øª",
    "ğŸ’š":"Ø­Ø¨",
    "ğŸ™":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’˜":"Ø­Ø¨",
    "ğŸƒ":"Ø³Ù„Ø§Ù…",
    "â˜º":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ¸":"Ø¶ÙØ¯Ø¹",
    "ğŸ˜¶":"Ù…ØµØ¯ÙˆÙ…",
    "âœŒï¸":"Ù…Ø±Ø­",
    "âœ‹ğŸ»":"ØªÙˆÙ‚Ù",
    "ğŸ˜‰":"ØºÙ…Ø²Ø©",
    "ğŸŒ·":"Ø­Ø¨",
    "ğŸ™ƒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜«":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜¨":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ¼ ":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸ":"Ù…Ø±Ø­",
    "ğŸ‚":"Ù…Ø±Ø­",
    "ğŸ’Ÿ":"Ø­Ø¨",
    "ğŸ˜ª":"Ø­Ø²Ù†",
    "ğŸ˜†":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜£":"Ø§Ø³ØªÙŠØ§Ø¡",
    "â˜ºï¸":"Ø­Ø¨",
    "ğŸ˜±":"ÙƒØ§Ø±Ø«Ø©",
    "ğŸ˜":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜–":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸƒğŸ¼":"ÙŠØ¬Ø±ÙŠ",
    "ğŸ˜¡":"ØºØ¶Ø¨",
    "ğŸš¶":"ÙŠØ³ÙŠØ±",
    "ğŸ¤•":"Ù…Ø±Ø¶",
    "â€¼ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ•Š":"Ø·Ø§Ø¦Ø±",
    "ğŸ‘ŒğŸ»":"Ø§Ø­Ø³Ù†Øª",
    "â£":"Ø­Ø¨",
    "ğŸ™Š":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’ƒ":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ’ƒğŸ¼":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ˜œ":"Ù…Ø±Ø­",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜Ÿ":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸ’–":"Ø­Ø¨",
    "ğŸ˜¥":"Ø­Ø²Ù†",
    "ğŸ»":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "âœ’":"ÙŠÙƒØªØ¨",
    "ğŸš¶ğŸ»":"ÙŠØ³ÙŠØ±",
    "ğŸ’":"Ø§Ù„Ù…Ø§Ø¸",
    "ğŸ˜·":"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶",
    "â˜":"ÙˆØ§Ø­Ø¯",
    "ğŸš¬":"ØªØ¯Ø®ÙŠÙ†",
    "ğŸ’" : "ÙˆØ±Ø¯",
    "ğŸŒ" : "Ø´Ù…Ø³",
    "ğŸ‘†" : "Ø§Ù„Ø§ÙˆÙ„",
    "âš ï¸" :"ØªØ­Ø°ÙŠØ±",
    "ğŸ¤—" : "Ø§Ø­ØªÙˆØ§Ø¡",
    "âœ–ï¸": "ØºÙ„Ø·",
    "ğŸ“"  : "Ù…ÙƒØ§Ù†",
    "ğŸ‘¸" : "Ù…Ù„ÙƒÙ‡",
    "ğŸ‘‘" : "ØªØ§Ø¬",
    "âœ”ï¸" : "ØµØ­",
    "ğŸ’Œ": "Ù‚Ù„Ø¨",
    "ğŸ˜²" : "Ù…Ù†Ø¯Ù‡Ø´",
    "ğŸ’¦": "Ù…Ø§Ø¡",
    "ğŸš«" : "Ø®Ø·Ø§",
    "ğŸ‘ğŸ»" : "Ø¨Ø±Ø§ÙÙˆ",
    "ğŸŠ" :"ÙŠØ³Ø¨Ø­",
    "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…",
    "â­•ï¸" :"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡",
    "ğŸ·" : "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
    "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯",
    "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸŒ":"Ù…Ø¨ØªØ³Ù…",
    "â¿"  : "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡",
    "ğŸ’ªğŸ¼" : "Ù‚ÙˆÙŠ",
    "ğŸ“©":  "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
    "â˜•ï¸": "Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜§" : "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©",
    "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©",
    "â—ï¸" :"ØªØ¹Ø¬Ø¨",
    "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡",
    "ğŸ‘¯" :"Ø§Ø®ÙˆØ§Øª",
    "Â©" :  "Ø±Ù…Ø²",
    "ğŸ‘µğŸ½" :"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡",
    "ğŸ£": "ÙƒØªÙƒÙˆØª",
    "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹",
    "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ",
    "ğŸ‘ğŸ½":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
    "â‰ï¸" : "Ø§Ø³ØªÙ†ÙƒØ§Ø±",
    "âš½ï¸": "ÙƒÙˆØ±Ù‡",
    "ğŸ•¶" :"Ø­Ø¨",
    "ğŸˆ" :"Ø¨Ø§Ù„ÙˆÙ†",
    "ğŸ€":    "ÙˆØ±Ø¯Ù‡",
    "ğŸ’µ":  "ÙÙ„ÙˆØ³",
    "ğŸ˜‹":  "Ø¬Ø§Ø¦Ø¹",
    "ğŸ˜›":  "ÙŠØºÙŠØ¸",
    "ğŸ˜ ":  "ØºØ§Ø¶Ø¨",
    "âœğŸ»":  "ÙŠÙƒØªØ¨",
    "ğŸŒ¾":  "Ø§Ø±Ø²",
    "ğŸ‘£":  "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†",
    "âŒ":"Ø±ÙØ¶",
    "ğŸŸ":"Ø·Ø¹Ø§Ù…",
    "ğŸ‘¬":"ØµØ¯Ø§Ù‚Ø©",
    "ğŸ°":"Ø§Ø±Ù†Ø¨",
    "â˜‚":"Ù…Ø·Ø±",
    "âšœ":"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§",
    "ğŸ‘":"Ø®Ø±ÙˆÙ",
    "ğŸ—£":"ØµÙˆØª Ù…Ø±ØªÙØ¹",
    "ğŸ‘ŒğŸ¼":"Ø§Ø­Ø³Ù†Øª",
    "â˜˜":"Ù…Ø±Ø­",
    "ğŸ˜®":"ØµØ¯Ù…Ø©",
    "ğŸ˜¦":"Ù‚Ù„Ù‚",
    "â­•":"Ø§Ù„Ø­Ù‚",
    "âœï¸":"Ù‚Ù„Ù…",
    "â„¹":"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
    "ğŸ™ğŸ»":"Ø±ÙØ¶",
    "âšªï¸":"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡",
    "ğŸ¤":"Ø­Ø²Ù†",
    "ğŸ’«":"Ù…Ø±Ø­",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ”":"Ø·Ø¹Ø§Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "âœˆï¸":"Ø³ÙØ±",
    "ğŸƒğŸ»â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ³":"Ø°ÙƒØ±",
    "ğŸ¤":"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡",
    "ğŸ¾":"ÙƒØ±Ù‡",
    "ğŸ”":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ™‹":"Ø³Ø¤Ø§Ù„",
    "ğŸ“®":"Ø¨Ø­Ø±",
    "ğŸ’‰":"Ø¯ÙˆØ§Ø¡",
    "ğŸ™ğŸ¼":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’‚ğŸ¿ ":"Ø­Ø§Ø±Ø³",
    "ğŸ¬":"Ø³ÙŠÙ†Ù…Ø§",
    "â™¦ï¸":"Ù…Ø±Ø­",
    "ğŸ’¡":"Ù‚ÙƒØ±Ø©",
    "â€¼":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘¼":"Ø·ÙÙ„",
    "ğŸ”‘":"Ù…ÙØªØ§Ø­",
    "â™¥ï¸":"Ø­Ø¨",
    "ğŸ•‹":"ÙƒØ¹Ø¨Ø©",
    "ğŸ“":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ’©":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸ‘½":"ÙØ¶Ø§Ø¦ÙŠ",
    "â˜”ï¸":"Ù…Ø·Ø±",
    "ğŸ·":"Ø¹ØµÙŠØ±",
    "ğŸŒŸ":"Ù†Ø¬Ù…Ø©",
    "â˜ï¸":"Ø³Ø­Ø¨",
    "ğŸ‘ƒ":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸŒº":"Ù…Ø±Ø­",
    "ğŸ”ª":"Ø³ÙƒÙŠÙ†Ø©",
    "â™¨":"Ø³Ø®ÙˆÙ†ÙŠØ©",
    "ğŸ‘ŠğŸ¼":"Ø¶Ø±Ø¨",
    "âœ":"Ù‚Ù„Ù…",
    "ğŸš¶ğŸ¾â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "â—¾ï¸":"ÙˆÙ‚Ù",
    "ğŸ˜š":"Ø­Ø¨",
    "ğŸ”¸":"Ù…Ø±Ø­",
    "ğŸ‘ğŸ»":"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ",
    "ğŸ‘ŠğŸ½":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜™":"Ø­Ø¨",
    "ğŸ¥":"ØªØµÙˆÙŠØ±",
    "ğŸ‘‰":"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡",
    "ğŸ‘ğŸ½":"ÙŠØµÙÙ‚",
    "ğŸ’ªğŸ»":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ´":"Ø§Ø³ÙˆØ¯",
    "ğŸ”¥":"Ø­Ø±ÙŠÙ‚",
    "ğŸ˜¬":"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©",
    "ğŸ‘ŠğŸ¿":"ÙŠØ¶Ø±Ø¨",
    "ğŸŒ¿":"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡",
    "âœ‹ğŸ¼":"ÙƒÙ Ø§ÙŠØ¯",
    "ğŸ‘":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "â˜ ï¸":"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨",
    "ğŸ‰":"ÙŠÙ‡Ù†Ø¦",
    "ğŸ”•" :"ØµØ§Ù…Øª",
    "ğŸ˜¿":"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†",
    "â˜¹ï¸":"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³",
    "ğŸ˜˜" :"Ø­Ø¨",
    "ğŸ˜°" :"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†",
    "ğŸŒ¼":"ÙˆØ±Ø¯Ù‡",
    "ğŸ’‹":  "Ø¨ÙˆØ³Ù‡",
    "ğŸ‘‡":"Ù„Ø§Ø³ÙÙ„",
    "â£ï¸":"Ø­Ø¨",
    "ğŸ§":"Ø³Ù…Ø§Ø¹Ø§Øª",
    "ğŸ“":"ÙŠÙƒØªØ¨",
    "ğŸ˜‡":"Ø¯Ø§ÙŠØ®",
    "ğŸ˜ˆ":"Ø±Ø¹Ø¨",
    "ğŸƒ":"ÙŠØ¬Ø±ÙŠ",
    "âœŒğŸ»":"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸ”«":"ÙŠØ¶Ø±Ø¨",
    "â—ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘":"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚",
    "ğŸ”":"Ù‚ÙÙ„",
    "ğŸ‘ˆ":"Ù„Ù„ÙŠÙ…ÙŠÙ†",
    "â„¢":"Ø±Ù…Ø²",
    "ğŸš¶ğŸ½":"ÙŠØªÙ…Ø´ÙŠ",
    "ğŸ˜¯":"Ù…ØªÙØ§Ø¬Ø£",
    "âœŠ":"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡",
    "ğŸ˜»":"Ø§Ø¹Ø¬Ø§Ø¨",
    "ğŸ™‰" :"Ù‚Ø±Ø¯",
    "ğŸ‘§":"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡",
    "ğŸ”´":"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡",
    "ğŸ’ªğŸ½":"Ù‚ÙˆÙ‡",
    "ğŸ’¤":"ÙŠÙ†Ø§Ù…",
    "ğŸ‘€":"ÙŠÙ†Ø¸Ø±",
    "âœğŸ»":"ÙŠÙƒØªØ¨",
    "â„ï¸":"ØªÙ„Ø¬",
    "ğŸ’€":"Ø±Ø¹Ø¨",
    "ğŸ˜¤":"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³",
    "ğŸ–‹":"Ù‚Ù„Ù…",
    "ğŸ©":"ÙƒØ§Ø¨",
    "â˜•ï¸":"Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜¹":"Ø¶Ø­Ùƒ",
    "ğŸ’“":"Ø­Ø¨",
    "â˜„ï¸ ":"Ù†Ø§Ø±",
    "ğŸ‘»":"Ø±Ø¹Ø¨",
    "â":"Ø®Ø·Ø¡",
    "ğŸ¤®":"Ø­Ø²Ù†",
    'ğŸ»':"Ø§Ø­Ù…Ø±"
    }

emoticons_to_emoji = {
    ":)" : "ğŸ™‚",
    ":(" : "ğŸ™",
    "xD" : "ğŸ˜†",
    ":=(": "ğŸ˜­",
    ":'(": "ğŸ˜¢",
    ":'â€‘(": "ğŸ˜¢",
    "XD" : "ğŸ˜‚",
    ":D" : "ğŸ™‚",
    "â™¬" : "Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "â™¡" : "â¤",
    "â˜»"  : "ğŸ™‚",
    }

diacritices = re.compile("""
                             Ù‘    | # Tashdid
                             Ù    | # Fatha
                             Ù‹    | # Tanwin Fath
                             Ù    | # Damma
                             ÙŒ    | # Tanwin Damm
                             Ù    | # Kasra
                             Ù    | # Tanwin Kasr
                             Ù’    | # Sukun
                             Ù€     # Tatwil/Kashida

                         """, re.VERBOSE)

def normalize_text(text):
  x=text
  text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
  text = re.sub("Ù‰", "ÙŠ", text)
  text = re.sub("Ø¤", "Ø¡", text)
  text = re.sub("Ø¦", "Ø¡", text)
  text = re.sub("Ø©", "Ù‡", text)
  text = re.sub("Ú¯", "Ùƒ", text)
  if text=='':
       text=x
  return text


def emojiTextTransform(text):
    cleantext = re.sub(r'[^\w\s]', ' ', text)
    emojistext = []

    for char in text:
        if char in emojis:
            emojistext.append(emojis[char])

    return cleantext + " ".join(emojistext)
def remove_redundant_spaces_number(text):
    text=re.sub(r'\b\d+\b', '', text)
    text = re.sub('[\u0660-\u0669]', '', text)
    text=re.sub(' +', ' ', text)
    text = re.sub(diacritices, '', text)
    return text

def remove_repeated_words_with_order(text):
    words = text.split()
    unique_words = []
    seen_words = set()

    for word in words:
        if word not in seen_words:
            unique_words.append(word)
            seen_words.add(word)

    cleaned_text = ' '.join(unique_words)
    return cleaned_text

def remove_repeating_chars(text):
    text = re.sub(r'(.)\1+', r'\1', text)
    return text

train_data['review_description'] = train_data['review_description'].apply(emojiTextTransform)
train_data['review_description'] = train_data['review_description'].apply(normalize_text)
train_data['review_description'] = train_data['review_description'].apply(remove_repeating_chars)
train_data['review_description'] = train_data['review_description'].apply(remove_repeated_words_with_order)
train_data['review_description'] = train_data['review_description'].apply(remove_redundant_spaces_number)

"""try approach for solving imbalance data by removing zero class


"""

import numpy as np

def encoding_for_two_classes__handle_imbalance(Y):
    encod = []
    for i in Y:
        if i == -1:
            encod.append(np.array([1, 0]))
        elif i == 1:
            encod.append(np.array([0, 1]))
    y_train = np.array(encod)
    return y_train



def encoding_for_three_classes__imbalance(Y):
    encod = []
    for i in Y:
        if i == -1:
            encod.append(np.array([1,0,0 ]))
        elif i == 0:
            encod.append(np.array([0,1,0]))
        elif i == 1:
            encod.append(np.array([0,0,1]))
    y_train = np.array(encod)
    return y_train

print(len(train_data))

x_train_with_imbalnce_with_3_classes=train_data["review_description"]
y_train_with_imbalnce_with_3_classes=encoding_for_three_classes__imbalance(train_data["rating"])
print(" ",x_train_with_imbalnce_with_3_classes.shape)
print("y_train_with_imbalnce_with_3_classes_shape",y_train_with_imbalnce_with_3_classes.shape)

class_1 = train_data[train_data['rating'] == 1]
class_minus_1 =train_data[train_data['rating'] == -1]
da= pd.concat([class_1, class_minus_1])
da = da.sample(frac=1, random_state=42).reset_index(drop=True)
x_train_with_imbalnce_with_2_classes=da["review_description"]
y_train_with_imbalnce_with_2_classes=encoding_for_two_classes__handle_imbalance(da["rating"])
print(da["rating"].value_counts())
print("x_train_with_imbalnce_with_2_classes_shape",x_train_with_imbalnce_with_2_classes.shape)
print("y_train_with_imbalnce_with_2_classes_shape",y_train_with_imbalnce_with_2_classes.shape)

# for experment three make balance between two classess  11000 sample from each class

from sklearn.utils import resample
class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]
#class_3 = train_data[train_data['rating'] ==0]
num_samples = 11000

undersampled_class_1 = resample(class_1, replace=False, n_samples=num_samples, random_state=42)
undersampled_class_minus_1 = resample(class_minus_1, replace=False, n_samples=num_samples, random_state=42)

undersampled_df = pd.concat([undersampled_class_1, undersampled_class_minus_1])

train_data_balance = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)

x_train_with_balnce_with_2_classes=train_data_balance["review_description"]

y_train_with_balnce_with_2_classes=encoding_for_two_classes__handle_imbalance(train_data_balance["rating"])

print(train_data_balance["rating"].value_counts())

print("x_train_with_balnce_with_2_classes.shape",x_train_with_balnce_with_2_classes.shape)

print("y_train_with_balnce_with_2_classes_shape",y_train_with_balnce_with_2_classes.shape)

#enocding for two classses
tokenizer = Tokenizer(num_words=10000)
def encode_covert_to_numbers(x_ttrain):
    tokenizer.fit_on_texts(x_ttrain)

    X_train_seq_balance_2_classes = tokenizer.texts_to_sequences(x_ttrain)

    X_train_pad_balance_with2_classes = pad_sequences(X_train_seq_balance_2_classes, maxlen=100)
X_train_pad_balance_with2_classes=encode_covert_to_numbers(y_train_with_balnce_with_2_classes)

def build_model(X_train_pad,Y_train,num_class):
  model = Sequential()
  model.add(Embedding(10000, 128, input_length=100))
  model.add(LSTM(128) )
  model.add(Dense(num_class, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

  model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])
  return model

#experment1
model=build_model(X_train_pad_balance_with2_classes,y_train_with_balnce_with_2_classes,2)

# testing experiment 1 two classes imbalance
test_data=pd.read_csv("/content/test _no_label.csv")
test_data['review_description'] = test_data['review_description'].apply(emojiTextTransform)
test_data['review_description'] = test_data['review_description'].apply(normalize_text)
test_data['review_description'] = test_data['review_description'].apply(remove_repeating_chars)
test_data['review_description'] = test_data['review_description'].apply(remove_repeated_words_with_order)
test_data['review_description'] = test_data['review_description'].apply(remove_redundant_spaces_number)

x_test_with_balnce_with_2_classes=test_data['review_description']

X_test_seq_balance_2_classes = tokenizer.texts_to_sequences(x_test_with_balnce_with_2_classes)
X_test_pad_balance_2_classes  = pad_sequences(X_test_seq_balance_2_classes, maxlen=100)

prediction_of_experemit1_balance=model.predict(X_test_pad_balance_2_classes)
print(prediction_of_experemit1_balance)

predicton_balance_2_classess=np.argmax(prediction_of_experemit1_balance,1)
for i in range(len(predicton_balance_2_classess)):
  if predicton_balance_2_classess[i]==0:
    predicton_balance_2_classess[i]=-1
  if predicton_balance_2_classess[i]==1:
        predicton_balance_2_classess[i]=1

ids=test_data["ID"]
submission=pd.DataFrame()
submission['ID']=test_data['ID']
submission['rating']=predicton_balance_2_classess
submission.to_csv("LSTM.csv",index = False)

#67

"""trying another apporach of preprocessing get high accauarcy


"""

train_data_with_different_preprocessing_its_get_high_acc=pd.read_excel("/content/train.xlsx")

# another apporach for prerprocessing wtihout stemming  just claen text(remove_punctuation,remove_stop_words,remove_numbers_A/E)
# and normlize it  handle if cell is null


from nltk.corpus import stopwords
nltk.download('stopwords')
punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€''' + string.punctuation
stop_words = stopwords.words('arabic')

def preprocess(text):
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)


    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ø¡", text)
    text = re.sub("Ø¦", "Ø¡", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("Ú¯", "Ùƒ", text)
    text = re.sub(r'\d+', '', text)
    #text = re.sub(r'[^Ø€-Û¿a-zA-Z\s]', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    re.sub(r'[Ù -Ù©]', '', text)


    text = ' '.join(word for word in text.split() if word not in stop_words)
    if text=='':
        text="Ø±ÙˆØ¯ÙŠÙ†Ø§"

    return text

# the same as prevaious experiment  hande
# 1 remove the class zero it is 4%
# 2 trying to make balance between  two others calss 1 and -1  and run experiment
# result its get 80% accuarcy

from sklearn.utils import resample
class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]
#class_3 = train_data[train_data['rating'] ==0]
num_samples = 11000

undersampled_class_1 = resample(class_1, replace=False, n_samples=num_samples, random_state=42)
undersampled_class_minus_1 = resample(class_minus_1, replace=False, n_samples=num_samples, random_state=42)

undersampled_df = pd.concat([undersampled_class_1, undersampled_class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]
X_train.head(5)
print(len(Y_train))
print(len(X_train))
import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)

print(Y_train.shape)




tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)


model = Sequential()
model.add(Embedding(10000, 128, input_length=100))
model.add(LSTM(128) ) # Added dropout for regularization
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

# testing on balance with_two_classess
test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)

prediction = np.argmax(prediction,axis = 1)
print(prediction)

for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya.csv', index=False)

#another approch on two class positive & negative with imbalance get result best from prevaious one
# resample on data

train_data=pd.read_excel("/content/train.xlsx")


from sklearn.utils import resample


class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]

undersampled_df = pd.concat([class_1, class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]

import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)




tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)


model = Sequential()
model.add(Embedding(10000, 128, input_length=100))
model.add(LSTM(128) )
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

# testing
test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)
prediction = np.argmax(prediction,axis = 1)
print(prediction)
for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya_last.csv', index=False)

import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout
from keras.callbacks import EarlyStopping



train_data=pd.read_excel("/content/train.xlsx")
from nltk.corpus import stopwords
nltk.download('stopwords')

punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€''' + string.punctuation
stop_words = stopwords.words('arabic')
def preprocess(text):
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)


    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ø¡", text)
    text = re.sub("Ø¦", "Ø¡", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("Ú¯", "Ùƒ", text)
    text = re.sub(r'\d+', '', text)
    #text = re.sub(r'[^Ø€-Û¿a-zA-Z\s]', '', text)
    text = re.sub(r'\b\d+\b', '', text)


    text = ' '.join(word for word in text.split() if word not in stop_words)
    if text=='':
        text="Ø­Ù†Ø§ÙˆÙ‰"





    return text

from sklearn.utils import resample

class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]

undersampled_df = pd.concat([class_1, class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]
X_train.head(5)
print(len(Y_train))
print(len(X_train))
import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)

print(Y_train.shape)


tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)



import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout

def transformer__classification_model(max_seq_length, vocab_size, num_heads=8, ff_dim=32, num_blocks=6, dropout_rate=0.1):
    inputs = Input(shape=(max_seq_length,))

    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(inputs)

    x = embedding_layer
    for _ in range(num_blocks):
        x = transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate)

    x = GlobalAveragePooling1D()(x)

    # Dense layers for binary classification
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.1)(x)
    outputs = Dense(2, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)

    return model

def transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate=0.1):
    # Multi-Head Self Attention
    multi_head_attention = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=256, dropout=dropout_rate
    )(x, x)

    # Add & Norm
    x = tf.keras.layers.Add()([x, multi_head_attention])
    x = tf.keras.layers.LayerNormalization()(x)

    # Feed Forward Part
    ff = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)
    ff = tf.keras.layers.Dropout(dropout_rate)(ff)
    ff = tf.keras.layers.Conv1D(filters=128, kernel_size=1)(ff)

    # Add & Norm
    x = tf.keras.layers.Add()([x, ff])
    x = tf.keras.layers.LayerNormalization()(x)

    return x

# Example usage:
max_seq_length = 100
vocab_size = 10000
num_heads = 4
ff_dim = 20
num_blocks = 3
dropout_rate = 0.1

model = transformer__classification_model(max_seq_length, vocab_size, num_heads, ff_dim, num_blocks, dropout_rate)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)
prediction = np.argmax(prediction,axis = 1)
#print(prediction)
for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya_last_last.csv', index=False)

# experments on three classses
x_train_with_imbalnce_with_3_classes=train_data["review_description"].apply(preprocess)
y_train_with_imbalnce_with_3_classes=encoding_for_three_classes__imbalance(train_data["rating"])
print("x_train_with_imbalnce_with_3_classes_shape",x_train_with_imbalnce_with_3_classes.shape)
print("y_train_with_imbalnce_with_3_classes_shape",y_train_with_imbalnce_with_3_classes.shape)

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(x_train_with_imbalnce_with_3_classes)

X_train_seq3 = tokenizer.texts_to_sequences(x_train_with_imbalnce_with_3_classes)
X_train_pad3 = pad_sequences(X_train_seq3, maxlen=100)

# note validation making on 20% from data in each epoch

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import LearningRateScheduler, EarlyStopping
import numpy as np





max_words =10000
embedding_dim = 100
lstm_units = 50
conv_filters = 100
kernel_size = 3
dense_units = 50
num_classes = 3
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=100))

model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))

model.add(Conv1D(filters=conv_filters, kernel_size=kernel_size, activation='relu'))

model.add(GlobalMaxPooling1D())

model.add(Dense(units=dense_units, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(units=num_classes, activation='softmax'))
initial_learning_rate = 0.001
optimizer = Adam(learning_rate=initial_learning_rate)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
batch_size = 150
epochs = 15
def lr_schedule(epoch, lr):
    return lr * 0.9 ** epoch
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)
history = model.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping, lr_scheduler])

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))

  # Plot learning rate schedule
plt.subplot(1, 3, 1)
plt.plot(range(1, 11), [lr_schedule(epoch, initial_learning_rate) for epoch in range(10)], marker='o')
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')

# Plot training loss
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

# Plot validation loss
plt.subplot(1, 3, 3)
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

test_data=pd.read_csv("/content/test _no_label.csv")
def test_function(model):
    """
     np.argmax it searches for max probability  in vector of prediction and get index of it like  [0.7,0.2,0.3]
     [0.7,0.2,0.3] the prediction of model the max prediction is 0.7 so it is convert to [1,0,0]
     it is negative class according to onehotenocding
      -1-->[1,0,0]-->0 index
      0-->[0,1,0]--> 1 index
      1-->[0,0,1]-->2 --> index

    """

    x_test_with_balnce_with_3_classes=test_data['review_description'].apply(preprocess)
    X_test_seq_balance_3_classes = tokenizer.texts_to_sequences(x_test_with_balnce_with_3_classes)
    X_test_pad_balance_3_classes  = pad_sequences(X_test_seq_balance_3_classes, maxlen=100)
    prediction = model.predict(X_test_pad_balance_3_classes)
    print(prediction)
    prediction = np.argmax(prediction,axis = 1)
    print(prediction)
    for i in range(0, len(prediction)):
      if prediction[i] == 0:
           prediction[i] = -1
      elif prediction[i] == 2:
           prediction[i] = 1
      else:
           prediction[i] = 0
    submission=pd.DataFrame()
    submission['ID']=test_data.ID
    submission['rating']=prediction
    submission.to_csv('submit3.csv', index=False)
    print(prediction)

#testing
test_function(model)

from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.callbacks import LearningRateScheduler, EarlyStopping
import numpy as np

model_rnn = Sequential()
model_rnn.add(Embedding(10000, 128, input_length=100))
model_rnn.add(SimpleRNN(50))
model_rnn.add(Dense(3, activation='softmax'))

initial_learning_rate = 0.001
optimizer = Adam(learning_rate=initial_learning_rate)
model_rnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model_rnn.summary()

batch_size = 150
epochs = 10

def lr_schedule(epoch, lr):
    return lr * 0.9 ** epoch



early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)

history = model_rnn.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping, lr_scheduler])

test_function(model_rnn)

def lr_schedule(epoch):
    if epoch < 5:
        return 0.0001
    elif epoch <10:
        return 0.001

lr_scheduler = LearningRateScheduler(lr_schedule)


def transformer_classification_model(max_seq_length, vocab_size, num_heads=8, ff_dim=32, num_blocks=6, dropout_rate=0.1):

    inputs = Input(shape=(max_seq_length,))

    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(inputs)

    x = embedding_layer
    for _ in range(num_blocks):
        x = transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate)

    x = GlobalAveragePooling1D()(x)

    x = Dense(64, activation='relu')(x)
    x = Dropout(0.1)(x)
    outputs = Dense(3, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)

    return model

def transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate=0.1):
    # Multi-Head Self Attention
    multi_head_attention = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=256, dropout=dropout_rate
    )(x, x)

    # Add & Norm
    x = tf.keras.layers.Add()([x, multi_head_attention])
    x = tf.keras.layers.LayerNormalization()(x)

    # Feed Forward Part
    ff = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)
    ff = tf.keras.layers.Dropout(dropout_rate)(ff)
    ff = tf.keras.layers.Conv1D(filters=128, kernel_size=1)(ff)

    # Add & Norm
    x = tf.keras.layers.Add()([x, ff])
    x = tf.keras.layers.LayerNormalization()(x)

    return x

max_seq_length = 100
vocab_size = 10000
num_heads = 7
ff_dim = 20
num_blocks = 3
dropout_rate = 0.1

model = transformer_classification_model(max_seq_length, vocab_size, num_heads, ff_dim, num_blocks, dropout_rate)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

h=model.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping,lr_scheduler])

test_function(model)

