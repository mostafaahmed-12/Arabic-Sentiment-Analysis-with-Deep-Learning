# -*- coding: utf-8 -*-
"""NeuralNetworkFinall.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16fXbSqI589yYmFSUSR6p7S1qheQVcIq3
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install NLTK

# %pip install Arabic-Stopwords

# %pip install clean-text

# %pip install demoji

from nltk import word_tokenize
from nltk import sent_tokenize
#import arabicstopwords.arabicstopwords as stp
from nltk.corpus import stopwords
#from cleantext import clean
from nltk.stem import ISRIStemmer
import pandas as pd
import re
import string
import sys
#import demoji
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
import pandas as pd
import re
#import emoji
from nltk.corpus import stopwords
import numpy as np
import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout
from keras.callbacks import EarlyStopping
import numpy as np
from keras.callbacks import LearningRateScheduler
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping

nltk.download('punkt')
nltk.download('stopwords')

excel_data = pd.read_excel('train.xlsx')
train_data = pd.DataFrame(excel_data,columns=['review_description', 'rating'])

ax = train_data['rating'].value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Class Distribution")

train_data.review_description.duplicated().sum()

train_data.review_description.isnull().sum()

train_data.drop(train_data[train_data.review_description.duplicated() == True].index, axis = 0, inplace = True)

train_data.review_description.duplicated().sum()

train_data.review_description=train_data.review_description.astype(str)
train_data.review_description=train_data.review_description.apply(lambda x:re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', x))
train_data.review_description=train_data.review_description.apply(lambda x:x.replace('؛',"", ))

train_data.head(5)

emojis = {
    "🙂":"يبتسم",
    "😂":"يضحك",
    "💔":"قلب حزين",
    "🙂":"يبتسم",
    "❤️":"حب",
    "❤":"حب",
    "😍":"حب",
    "😭":"يبكي",
    "😢":"حزن",
    "😔":"حزن",
    "♥":"حب",
    "💜":"حب",
    "😅":"يضحك",
    "🙁":"حزين",
    "💕":"حب",
    "💙":"حب",
    "😞":"حزين",
    "😊":"سعادة",
    "👏":"يصفق",
    "👌":"احسنت",
    "😴":"ينام",
    "😀":"يضحك",
    "😌":"حزين",
    "🌹":"وردة",
    "🙈":"حب",
    "😄":"يضحك",
    "😐":"محايد",
    "✌":"منتصر",
    "✨":"نجمه",
    "🤔":"تفكير",
    "😏":"يستهزء",
    "😒":"يستهزء",
    "🙄":"ملل",
    "😕":"عصبية",
    "😃":"يضحك",
    "🌸":"وردة",
    "😓":"حزن",
    "💞":"حب",
    "💗":"حب",
    "😑":"منزعج",
    "💭":"تفكير",
    "😎":"ثقة",
    "💛":"حب",
    "😩":"حزين",
    "💪":"عضلات",
    "👍":"موافق",
    "🙏🏻":"رجاء طلب",
    "😳":"مصدوم",
    "👏🏼":"تصفيق",
    "🎶":"موسيقي",
    "🌚":"صمت",
    "💚":"حب",
    "🙏":"رجاء طلب",
    "💘":"حب",
    "🍃":"سلام",
    "☺":"يضحك",
    "🐸":"ضفدع",
    "😶":"مصدوم",
    "✌️":"مرح",
    "✋🏻":"توقف",
    "😉":"غمزة",
    "🌷":"حب",
    "🙃":"مبتسم",
    "😫":"حزين",
    "😨":"مصدوم",
    "🎼 ":"موسيقي",
    "🍁":"مرح",
    "🍂":"مرح",
    "💟":"حب",
    "😪":"حزن",
    "😆":"يضحك",
    "😣":"استياء",
    "☺️":"حب",
    "😱":"كارثة",
    "😁":"يضحك",
    "😖":"استياء",
    "🏃🏼":"يجري",
    "😡":"غضب",
    "🚶":"يسير",
    "🤕":"مرض",
    "‼️":"تعجب",
    "🕊":"طائر",
    "👌🏻":"احسنت",
    "❣":"حب",
    "🙊":"مصدوم",
    "💃":"سعادة مرح",
    "💃🏼":"سعادة مرح",
    "😜":"مرح",
    "👊":"ضربة",
    "😟":"استياء",
    "💖":"حب",
    "😥":"حزن",
    "🎻":"موسيقي",
    "✒":"يكتب",
    "🚶🏻":"يسير",
    "💎":"الماظ",
    "😷":"وباء مرض",
    "☝":"واحد",
    "🚬":"تدخين",
    "💐" : "ورد",
    "🌞" : "شمس",
    "👆" : "الاول",
    "⚠️" :"تحذير",
    "🤗" : "احتواء",
    "✖️": "غلط",
    "📍"  : "مكان",
    "👸" : "ملكه",
    "👑" : "تاج",
    "✔️" : "صح",
    "💌": "قلب",
    "😲" : "مندهش",
    "💦": "ماء",
    "🚫" : "خطا",
    "👏🏻" : "برافو",
    "🏊" :"يسبح",
    "👍🏻": "تمام",
    "⭕️" :"دائره كبيره",
    "🎷" : "ساكسفون",
    "👋": "تلويح باليد",
    "✌🏼": "علامه النصر",
    "🌝":"مبتسم",
    "➿"  : "عقده مزدوجه",
    "💪🏼" : "قوي",
    "📩":  "تواصل معي",
    "☕️": "قهوه",
    "😧" : "قلق و صدمة",
    "🗨": "رسالة",
    "❗️" :"تعجب",
    "🙆🏻": "اشاره موافقه",
    "👯" :"اخوات",
    "©" :  "رمز",
    "👵🏽" :"سيده عجوزه",
    "🐣": "كتكوت",
    "🙌": "تشجيع",
    "🙇": "شخص ينحني",
    "👐🏽":"ايدي مفتوحه",
    "👌🏽": "بالظبط",
    "⁉️" : "استنكار",
    "⚽️": "كوره",
    "🕶" :"حب",
    "🎈" :"بالون",
    "🎀":    "ورده",
    "💵":  "فلوس",
    "😋":  "جائع",
    "😛":  "يغيظ",
    "😠":  "غاضب",
    "✍🏻":  "يكتب",
    "🌾":  "ارز",
    "👣":  "اثر قدمين",
    "❌":"رفض",
    "🍟":"طعام",
    "👬":"صداقة",
    "🐰":"ارنب",
    "☂":"مطر",
    "⚜":"مملكة فرنسا",
    "🐑":"خروف",
    "🗣":"صوت مرتفع",
    "👌🏼":"احسنت",
    "☘":"مرح",
    "😮":"صدمة",
    "😦":"قلق",
    "⭕":"الحق",
    "✏️":"قلم",
    "ℹ":"معلومات",
    "🙍🏻":"رفض",
    "⚪️":"نضارة نقاء",
    "🐤":"حزن",
    "💫":"مرح",
    "💝":"حب",
    "🍔":"طعام",
    "❤︎":"حب",
    "✈️":"سفر",
    "🏃🏻‍♀️":"يسير",
    "🍳":"ذكر",
    "🎤":"مايك غناء",
    "🎾":"كره",
    "🐔":"دجاجة",
    "🙋":"سؤال",
    "📮":"بحر",
    "💉":"دواء",
    "🙏🏼":"رجاء طلب",
    "💂🏿 ":"حارس",
    "🎬":"سينما",
    "♦️":"مرح",
    "💡":"قكرة",
    "‼":"تعجب",
    "👼":"طفل",
    "🔑":"مفتاح",
    "♥️":"حب",
    "🕋":"كعبة",
    "🐓":"دجاجة",
    "💩":"معترض",
    "👽":"فضائي",
    "☔️":"مطر",
    "🍷":"عصير",
    "🌟":"نجمة",
    "☁️":"سحب",
    "👃":"معترض",
    "🌺":"مرح",
    "🔪":"سكينة",
    "♨":"سخونية",
    "👊🏼":"ضرب",
    "✏":"قلم",
    "🚶🏾‍♀️":"يسير",
    "👊":"ضربة",
    "◾️":"وقف",
    "😚":"حب",
    "🔸":"مرح",
    "👎🏻":"لا يعجبني",
    "👊🏽":"ضربة",
    "😙":"حب",
    "🎥":"تصوير",
    "👉":"جذب انتباه",
    "👏🏽":"يصفق",
    "💪🏻":"عضلات",
    "🏴":"اسود",
    "🔥":"حريق",
    "😬":"عدم الراحة",
    "👊🏿":"يضرب",
    "🌿":"ورقه شجره",
    "✋🏼":"كف ايد",
    "👐":"ايدي مفتوحه",
    "☠️":"وجه مرعب",
    "🎉":"يهنئ",
    "🔕" :"صامت",
    "😿":"وجه حزين",
    "☹️":"وجه يائس",
    "😘" :"حب",
    "😰" :"خوف و حزن",
    "🌼":"ورده",
    "💋":  "بوسه",
    "👇":"لاسفل",
    "❣️":"حب",
    "🎧":"سماعات",
    "📝":"يكتب",
    "😇":"دايخ",
    "😈":"رعب",
    "🏃":"يجري",
    "✌🏻":"علامه النصر",
    "🔫":"يضرب",
    "❗️":"تعجب",
    "👎":"غير موافق",
    "🔐":"قفل",
    "👈":"لليمين",
    "™":"رمز",
    "🚶🏽":"يتمشي",
    "😯":"متفاجأ",
    "✊":"يد مغلقه",
    "😻":"اعجاب",
    "🙉" :"قرد",
    "👧":"طفله صغيره",
    "🔴":"دائره حمراء",
    "💪🏽":"قوه",
    "💤":"ينام",
    "👀":"ينظر",
    "✍🏻":"يكتب",
    "❄️":"تلج",
    "💀":"رعب",
    "😤":"وجه عابس",
    "🖋":"قلم",
    "🎩":"كاب",
    "☕️":"قهوه",
    "😹":"ضحك",
    "💓":"حب",
    "☄️ ":"نار",
    "👻":"رعب",
    "❎":"خطء",
    "🤮":"حزن",
    '🏻':"احمر"
    }

emoticons_to_emoji = {
    ":)" : "🙂",
    ":(" : "🙁",
    "xD" : "😆",
    ":=(": "😭",
    ":'(": "😢",
    ":'‑(": "😢",
    "XD" : "😂",
    ":D" : "🙂",
    "♬" : "موسيقي",
    "♡" : "❤",
    "☻"  : "🙂",
    }

diacritices = re.compile("""
                             ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida

                         """, re.VERBOSE)

def normalize_text(text):
  x=text
  text = re.sub("[إأآا]", "ا", text)
  text = re.sub("ى", "ي", text)
  text = re.sub("ؤ", "ء", text)
  text = re.sub("ئ", "ء", text)
  text = re.sub("ة", "ه", text)
  text = re.sub("گ", "ك", text)
  if text=='':
       text=x
  return text


def emojiTextTransform(text):
    cleantext = re.sub(r'[^\w\s]', ' ', text)
    emojistext = []

    for char in text:
        if char in emojis:
            emojistext.append(emojis[char])

    return cleantext + " ".join(emojistext)
def remove_redundant_spaces_number(text):
    text=re.sub(r'\b\d+\b', '', text)
    text = re.sub('[\u0660-\u0669]', '', text)
    text=re.sub(' +', ' ', text)
    text = re.sub(diacritices, '', text)
    return text

def remove_repeated_words_with_order(text):
    words = text.split()
    unique_words = []
    seen_words = set()

    for word in words:
        if word not in seen_words:
            unique_words.append(word)
            seen_words.add(word)

    cleaned_text = ' '.join(unique_words)
    return cleaned_text

def remove_repeating_chars(text):
    text = re.sub(r'(.)\1+', r'\1', text)
    return text

train_data['review_description'] = train_data['review_description'].apply(emojiTextTransform)
train_data['review_description'] = train_data['review_description'].apply(normalize_text)
train_data['review_description'] = train_data['review_description'].apply(remove_repeating_chars)
train_data['review_description'] = train_data['review_description'].apply(remove_repeated_words_with_order)
train_data['review_description'] = train_data['review_description'].apply(remove_redundant_spaces_number)

"""try approach for solving imbalance data by removing zero class


"""

import numpy as np

def encoding_for_two_classes__handle_imbalance(Y):
    encod = []
    for i in Y:
        if i == -1:
            encod.append(np.array([1, 0]))
        elif i == 1:
            encod.append(np.array([0, 1]))
    y_train = np.array(encod)
    return y_train



def encoding_for_three_classes__imbalance(Y):
    encod = []
    for i in Y:
        if i == -1:
            encod.append(np.array([1,0,0 ]))
        elif i == 0:
            encod.append(np.array([0,1,0]))
        elif i == 1:
            encod.append(np.array([0,0,1]))
    y_train = np.array(encod)
    return y_train

print(len(train_data))

x_train_with_imbalnce_with_3_classes=train_data["review_description"]
y_train_with_imbalnce_with_3_classes=encoding_for_three_classes__imbalance(train_data["rating"])
print(" ",x_train_with_imbalnce_with_3_classes.shape)
print("y_train_with_imbalnce_with_3_classes_shape",y_train_with_imbalnce_with_3_classes.shape)

class_1 = train_data[train_data['rating'] == 1]
class_minus_1 =train_data[train_data['rating'] == -1]
da= pd.concat([class_1, class_minus_1])
da = da.sample(frac=1, random_state=42).reset_index(drop=True)
x_train_with_imbalnce_with_2_classes=da["review_description"]
y_train_with_imbalnce_with_2_classes=encoding_for_two_classes__handle_imbalance(da["rating"])
print(da["rating"].value_counts())
print("x_train_with_imbalnce_with_2_classes_shape",x_train_with_imbalnce_with_2_classes.shape)
print("y_train_with_imbalnce_with_2_classes_shape",y_train_with_imbalnce_with_2_classes.shape)

# for experment three make balance between two classess  11000 sample from each class

from sklearn.utils import resample
class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]
#class_3 = train_data[train_data['rating'] ==0]
num_samples = 11000

undersampled_class_1 = resample(class_1, replace=False, n_samples=num_samples, random_state=42)
undersampled_class_minus_1 = resample(class_minus_1, replace=False, n_samples=num_samples, random_state=42)

undersampled_df = pd.concat([undersampled_class_1, undersampled_class_minus_1])

train_data_balance = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)

x_train_with_balnce_with_2_classes=train_data_balance["review_description"]

y_train_with_balnce_with_2_classes=encoding_for_two_classes__handle_imbalance(train_data_balance["rating"])

print(train_data_balance["rating"].value_counts())

print("x_train_with_balnce_with_2_classes.shape",x_train_with_balnce_with_2_classes.shape)

print("y_train_with_balnce_with_2_classes_shape",y_train_with_balnce_with_2_classes.shape)

#enocding for two classses
tokenizer = Tokenizer(num_words=10000)
def encode_covert_to_numbers(x_ttrain):
    tokenizer.fit_on_texts(x_ttrain)

    X_train_seq_balance_2_classes = tokenizer.texts_to_sequences(x_ttrain)

    X_train_pad_balance_with2_classes = pad_sequences(X_train_seq_balance_2_classes, maxlen=100)
X_train_pad_balance_with2_classes=encode_covert_to_numbers(y_train_with_balnce_with_2_classes)

def build_model(X_train_pad,Y_train,num_class):
  model = Sequential()
  model.add(Embedding(10000, 128, input_length=100))
  model.add(LSTM(128) )
  model.add(Dense(num_class, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

  model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])
  return model

#experment1
model=build_model(X_train_pad_balance_with2_classes,y_train_with_balnce_with_2_classes,2)

# testing experiment 1 two classes imbalance
test_data=pd.read_csv("/content/test _no_label.csv")
test_data['review_description'] = test_data['review_description'].apply(emojiTextTransform)
test_data['review_description'] = test_data['review_description'].apply(normalize_text)
test_data['review_description'] = test_data['review_description'].apply(remove_repeating_chars)
test_data['review_description'] = test_data['review_description'].apply(remove_repeated_words_with_order)
test_data['review_description'] = test_data['review_description'].apply(remove_redundant_spaces_number)

x_test_with_balnce_with_2_classes=test_data['review_description']

X_test_seq_balance_2_classes = tokenizer.texts_to_sequences(x_test_with_balnce_with_2_classes)
X_test_pad_balance_2_classes  = pad_sequences(X_test_seq_balance_2_classes, maxlen=100)

prediction_of_experemit1_balance=model.predict(X_test_pad_balance_2_classes)
print(prediction_of_experemit1_balance)

predicton_balance_2_classess=np.argmax(prediction_of_experemit1_balance,1)
for i in range(len(predicton_balance_2_classess)):
  if predicton_balance_2_classess[i]==0:
    predicton_balance_2_classess[i]=-1
  if predicton_balance_2_classess[i]==1:
        predicton_balance_2_classess[i]=1

ids=test_data["ID"]
submission=pd.DataFrame()
submission['ID']=test_data['ID']
submission['rating']=predicton_balance_2_classess
submission.to_csv("LSTM.csv",index = False)

#67

"""trying another apporach of preprocessing get high accauarcy


"""

train_data_with_different_preprocessing_its_get_high_acc=pd.read_excel("/content/train.xlsx")

# another apporach for prerprocessing wtihout stemming  just claen text(remove_punctuation,remove_stop_words,remove_numbers_A/E)
# and normlize it  handle if cell is null


from nltk.corpus import stopwords
nltk.download('stopwords')
punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation
stop_words = stopwords.words('arabic')

def preprocess(text):
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)


    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    text = re.sub(r'\d+', '', text)
    #text = re.sub(r'[^؀-ۿa-zA-Z\s]', '', text)
    text = re.sub(r'\b\d+\b', '', text)
    re.sub(r'[٠-٩]', '', text)


    text = ' '.join(word for word in text.split() if word not in stop_words)
    if text=='':
        text="رودينا"

    return text

# the same as prevaious experiment  hande
# 1 remove the class zero it is 4%
# 2 trying to make balance between  two others calss 1 and -1  and run experiment
# result its get 80% accuarcy

from sklearn.utils import resample
class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]
#class_3 = train_data[train_data['rating'] ==0]
num_samples = 11000

undersampled_class_1 = resample(class_1, replace=False, n_samples=num_samples, random_state=42)
undersampled_class_minus_1 = resample(class_minus_1, replace=False, n_samples=num_samples, random_state=42)

undersampled_df = pd.concat([undersampled_class_1, undersampled_class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]
X_train.head(5)
print(len(Y_train))
print(len(X_train))
import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)

print(Y_train.shape)




tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)


model = Sequential()
model.add(Embedding(10000, 128, input_length=100))
model.add(LSTM(128) ) # Added dropout for regularization
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

# testing on balance with_two_classess
test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)

prediction = np.argmax(prediction,axis = 1)
print(prediction)

for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya.csv', index=False)

#another approch on two class positive & negative with imbalance get result best from prevaious one
# resample on data

train_data=pd.read_excel("/content/train.xlsx")


from sklearn.utils import resample


class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]

undersampled_df = pd.concat([class_1, class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]

import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)




tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)


model = Sequential()
model.add(Embedding(10000, 128, input_length=100))
model.add(LSTM(128) )
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

# testing
test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)
prediction = np.argmax(prediction,axis = 1)
print(prediction)
for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya_last.csv', index=False)

import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout
from keras.callbacks import EarlyStopping



train_data=pd.read_excel("/content/train.xlsx")
from nltk.corpus import stopwords
nltk.download('stopwords')

punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation
stop_words = stopwords.words('arabic')
def preprocess(text):
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)


    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    text = re.sub(r'\d+', '', text)
    #text = re.sub(r'[^؀-ۿa-zA-Z\s]', '', text)
    text = re.sub(r'\b\d+\b', '', text)


    text = ' '.join(word for word in text.split() if word not in stop_words)
    if text=='':
        text="حناوى"





    return text

from sklearn.utils import resample

class_1 = train_data[train_data['rating'] == 1]
class_minus_1 = train_data[train_data['rating'] == -1]

undersampled_df = pd.concat([class_1, class_minus_1])

train_data = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(train_data["rating"].value_counts())

X_train=train_data["review_description"].apply(preprocess)
Y_train=train_data["rating"]
X_train.head(5)
print(len(Y_train))
print(len(X_train))
import numpy as np
encod=[]
for i in Y_train:
    if i==-1:
        encod.append(np.array([1,0]))
    if i==1:
        encod.append(np.array([0,1]))

    if i==2:
        encod.append(np.array([0,0,1]))
Y_train=encod
Y_train=np.array(Y_train)

print(Y_train.shape)


tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)



import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout

def transformer__classification_model(max_seq_length, vocab_size, num_heads=8, ff_dim=32, num_blocks=6, dropout_rate=0.1):
    inputs = Input(shape=(max_seq_length,))

    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(inputs)

    x = embedding_layer
    for _ in range(num_blocks):
        x = transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate)

    x = GlobalAveragePooling1D()(x)

    # Dense layers for binary classification
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.1)(x)
    outputs = Dense(2, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)

    return model

def transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate=0.1):
    # Multi-Head Self Attention
    multi_head_attention = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=256, dropout=dropout_rate
    )(x, x)

    # Add & Norm
    x = tf.keras.layers.Add()([x, multi_head_attention])
    x = tf.keras.layers.LayerNormalization()(x)

    # Feed Forward Part
    ff = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)
    ff = tf.keras.layers.Dropout(dropout_rate)(ff)
    ff = tf.keras.layers.Conv1D(filters=128, kernel_size=1)(ff)

    # Add & Norm
    x = tf.keras.layers.Add()([x, ff])
    x = tf.keras.layers.LayerNormalization()(x)

    return x

# Example usage:
max_seq_length = 100
vocab_size = 10000
num_heads = 4
ff_dim = 20
num_blocks = 3
dropout_rate = 0.1

model = transformer__classification_model(max_seq_length, vocab_size, num_heads, ff_dim, num_blocks, dropout_rate)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
model.fit(X_train_pad, Y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping])

test_data=pd.read_csv("/content/test _no_label.csv")
ids=test_data['ID']
X_test=test_data['review_description']
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)


prediction = model.predict(X_test_pad)
prediction = np.argmax(prediction,axis = 1)
#print(prediction)
for i in range(0,len(prediction)):
    if prediction[i]==0:
        prediction[i]=-1
    else:
        prediction[i]=1

submission=pd.DataFrame()
submission['ID']=ids
submission['rating']=prediction
submission.to_csv('Aya_last_last.csv', index=False)

# experments on three classses
x_train_with_imbalnce_with_3_classes=train_data["review_description"].apply(preprocess)
y_train_with_imbalnce_with_3_classes=encoding_for_three_classes__imbalance(train_data["rating"])
print("x_train_with_imbalnce_with_3_classes_shape",x_train_with_imbalnce_with_3_classes.shape)
print("y_train_with_imbalnce_with_3_classes_shape",y_train_with_imbalnce_with_3_classes.shape)

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(x_train_with_imbalnce_with_3_classes)

X_train_seq3 = tokenizer.texts_to_sequences(x_train_with_imbalnce_with_3_classes)
X_train_pad3 = pad_sequences(X_train_seq3, maxlen=100)

# note validation making on 20% from data in each epoch

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import LearningRateScheduler, EarlyStopping
import numpy as np





max_words =10000
embedding_dim = 100
lstm_units = 50
conv_filters = 100
kernel_size = 3
dense_units = 50
num_classes = 3
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=100))

model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))

model.add(Conv1D(filters=conv_filters, kernel_size=kernel_size, activation='relu'))

model.add(GlobalMaxPooling1D())

model.add(Dense(units=dense_units, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(units=num_classes, activation='softmax'))
initial_learning_rate = 0.001
optimizer = Adam(learning_rate=initial_learning_rate)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
batch_size = 150
epochs = 15
def lr_schedule(epoch, lr):
    return lr * 0.9 ** epoch
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)
history = model.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping, lr_scheduler])

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))

  # Plot learning rate schedule
plt.subplot(1, 3, 1)
plt.plot(range(1, 11), [lr_schedule(epoch, initial_learning_rate) for epoch in range(10)], marker='o')
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')

# Plot training loss
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

# Plot validation loss
plt.subplot(1, 3, 3)
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

test_data=pd.read_csv("/content/test _no_label.csv")
def test_function(model):
    """
     np.argmax it searches for max probability  in vector of prediction and get index of it like  [0.7,0.2,0.3]
     [0.7,0.2,0.3] the prediction of model the max prediction is 0.7 so it is convert to [1,0,0]
     it is negative class according to onehotenocding
      -1-->[1,0,0]-->0 index
      0-->[0,1,0]--> 1 index
      1-->[0,0,1]-->2 --> index

    """

    x_test_with_balnce_with_3_classes=test_data['review_description'].apply(preprocess)
    X_test_seq_balance_3_classes = tokenizer.texts_to_sequences(x_test_with_balnce_with_3_classes)
    X_test_pad_balance_3_classes  = pad_sequences(X_test_seq_balance_3_classes, maxlen=100)
    prediction = model.predict(X_test_pad_balance_3_classes)
    print(prediction)
    prediction = np.argmax(prediction,axis = 1)
    print(prediction)
    for i in range(0, len(prediction)):
      if prediction[i] == 0:
           prediction[i] = -1
      elif prediction[i] == 2:
           prediction[i] = 1
      else:
           prediction[i] = 0
    submission=pd.DataFrame()
    submission['ID']=test_data.ID
    submission['rating']=prediction
    submission.to_csv('submit3.csv', index=False)
    print(prediction)

#testing
test_function(model)

from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.callbacks import LearningRateScheduler, EarlyStopping
import numpy as np

model_rnn = Sequential()
model_rnn.add(Embedding(10000, 128, input_length=100))
model_rnn.add(SimpleRNN(50))
model_rnn.add(Dense(3, activation='softmax'))

initial_learning_rate = 0.001
optimizer = Adam(learning_rate=initial_learning_rate)
model_rnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model_rnn.summary()

batch_size = 150
epochs = 10

def lr_schedule(epoch, lr):
    return lr * 0.9 ** epoch



early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)

history = model_rnn.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping, lr_scheduler])

test_function(model_rnn)

def lr_schedule(epoch):
    if epoch < 5:
        return 0.0001
    elif epoch <10:
        return 0.001

lr_scheduler = LearningRateScheduler(lr_schedule)


def transformer_classification_model(max_seq_length, vocab_size, num_heads=8, ff_dim=32, num_blocks=6, dropout_rate=0.1):

    inputs = Input(shape=(max_seq_length,))

    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(inputs)

    x = embedding_layer
    for _ in range(num_blocks):
        x = transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate)

    x = GlobalAveragePooling1D()(x)

    x = Dense(64, activation='relu')(x)
    x = Dropout(0.1)(x)
    outputs = Dense(3, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=outputs)

    return model

def transformer_block(x, max_seq_length, num_heads, ff_dim, dropout_rate=0.1):
    # Multi-Head Self Attention
    multi_head_attention = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=256, dropout=dropout_rate
    )(x, x)

    # Add & Norm
    x = tf.keras.layers.Add()([x, multi_head_attention])
    x = tf.keras.layers.LayerNormalization()(x)

    # Feed Forward Part
    ff = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)
    ff = tf.keras.layers.Dropout(dropout_rate)(ff)
    ff = tf.keras.layers.Conv1D(filters=128, kernel_size=1)(ff)

    # Add & Norm
    x = tf.keras.layers.Add()([x, ff])
    x = tf.keras.layers.LayerNormalization()(x)

    return x

max_seq_length = 100
vocab_size = 10000
num_heads = 7
ff_dim = 20
num_blocks = 3
dropout_rate = 0.1

model = transformer_classification_model(max_seq_length, vocab_size, num_heads, ff_dim, num_blocks, dropout_rate)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

h=model.fit(X_train_pad3, y_train_with_imbalnce_with_3_classes, validation_split=0.2, epochs=15, batch_size=32, callbacks=[early_stopping,lr_scheduler])

test_function(model)

